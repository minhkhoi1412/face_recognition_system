{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Compose, ToTensor, RandomHorizontalFlip, Normalize, Resize, RandomRotation\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from modules.deep_pixel_wise.Dataset_v2 import PixWiseDataset\n",
    "from modules.deep_pixel_wise.Model import DeePixBiS\n",
    "from modules.deep_pixel_wise.Loss import PixWiseBCELoss\n",
    "from modules.deep_pixel_wise.Metrics import predict, test_accuracy, test_loss\n",
    "from modules.deep_pixel_wise.Trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('.\\data\\\\path_files\\\\train_data_celeb_nuaa.csv').reset_index(drop=True)\n",
    "test_data = pd.read_csv('.\\data\\\\path_files\\\\test_data_celeb_nuaa.csv').reset_index(drop=True)\n",
    "val_data = pd.read_csv('.\\data\\\\path_files\\\\val_data_celeb_nuaa.csv').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\KhoiNXM\\Workspace\\Learning\\Master Thesis\\De...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\KhoiNXM\\Workspace\\Learning\\Master Thesis\\De...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\KhoiNXM\\Workspace\\Learning\\Master Thesis\\De...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\KhoiNXM\\Workspace\\Learning\\Master Thesis\\De...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\KhoiNXM\\Workspace\\Learning\\Master Thesis\\De...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  label\n",
       "0  C:\\KhoiNXM\\Workspace\\Learning\\Master Thesis\\De...      1\n",
       "1  C:\\KhoiNXM\\Workspace\\Learning\\Master Thesis\\De...      0\n",
       "2  C:\\KhoiNXM\\Workspace\\Learning\\Master Thesis\\De...      0\n",
       "3  C:\\KhoiNXM\\Workspace\\Learning\\Master Thesis\\De...      0\n",
       "4  C:\\KhoiNXM\\Workspace\\Learning\\Master Thesis\\De...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_shuffled = train_data.sample(frac=1).reset_index(drop=True)\n",
    "train_data_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7022, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_shuffled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(878, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(878, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3511"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(train_data_shuffled)*0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\KhoiNXM\\Workspace\\Learning\\Master Thesis\\De...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\KhoiNXM\\Workspace\\Learning\\Master Thesis\\De...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\KhoiNXM\\Workspace\\Learning\\Master Thesis\\De...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\KhoiNXM\\Workspace\\Learning\\Master Thesis\\De...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\KhoiNXM\\Workspace\\Learning\\Master Thesis\\De...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  label\n",
       "0  C:\\KhoiNXM\\Workspace\\Learning\\Master Thesis\\De...      0\n",
       "1  C:\\KhoiNXM\\Workspace\\Learning\\Master Thesis\\De...      0\n",
       "2  C:\\KhoiNXM\\Workspace\\Learning\\Master Thesis\\De...      1\n",
       "3  C:\\KhoiNXM\\Workspace\\Learning\\Master Thesis\\De...      0\n",
       "4  C:\\KhoiNXM\\Workspace\\Learning\\Master Thesis\\De...      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_train_data = train_data_shuffled[:int(len(train_data_shuffled)*0.5)].reset_index(drop=True)\n",
    "sub_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA T1200 Laptop GPU\n",
      "Memory Usage:\n",
      "Allocated:  0.0 GB\n",
      "Cached:  0.0 GB\n",
      "Max memory reserved:  0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Setting device on GPU if available, else CPU\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "# Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated: ', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached: ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "    print('Max memory reserved: ', round(torch.cuda.max_memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\KhoiNXM\\Workspace\\python_venv\\face_recognition_system\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\KhoiNXM\\Workspace\\python_venv\\face_recognition_system\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet161_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet161_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Beginning\n",
      "\n",
      "\n",
      "Epoch (1/5)\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\KhoiNXM\\Workspace\\python_venv\\face_recognition_system\\lib\\site-packages\\torch\\nn\\functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 0.7216019630432129\n",
      "Loss : 0.6919922828674316\n",
      "Loss : 0.6534020304679871\n",
      "Loss : 0.5962741374969482\n",
      "Loss : 0.5894075632095337\n",
      "Loss : 0.5867011547088623\n",
      "Loss : 0.5250152945518494\n",
      "Loss : 0.6019400954246521\n",
      "Loss : 0.4583001732826233\n",
      "Loss : 0.4993170499801636\n",
      "Loss : 0.4792345464229584\n",
      "Loss : 0.43064868450164795\n",
      "Loss : 0.5524972677230835\n",
      "Loss : 0.41495639085769653\n",
      "Loss : 0.4062007665634155\n",
      "Loss : 0.38776296377182007\n",
      "Loss : 0.47403717041015625\n",
      "Loss : 0.5294344425201416\n",
      "Loss : 0.41187718510627747\n",
      "Loss : 0.47110819816589355\n",
      "Loss : 0.48139506578445435\n",
      "Loss : 0.4879840612411499\n",
      "Loss : 0.40487638115882874\n",
      "Loss : 0.6626325845718384\n",
      "Loss : 0.42867928743362427\n",
      "Loss : 0.3231564462184906\n",
      "Loss : 0.3230980634689331\n",
      "Loss : 0.339822381734848\n",
      "Loss : 0.4657154679298401\n",
      "Loss : 0.3544003665447235\n",
      "Loss : 0.2743053734302521\n",
      "Loss : 0.3423230051994324\n",
      "Loss : 0.2826114892959595\n",
      "Loss : 0.28815191984176636\n",
      "Loss : 0.24593478441238403\n",
      "Loss : 0.32913026213645935\n",
      "Loss : 0.2469160556793213\n",
      "Loss : 0.2647349536418915\n",
      "Loss : 0.2697199583053589\n",
      "Loss : 0.23427289724349976\n",
      "Loss : 0.366890013217926\n",
      "Loss : 0.41186684370040894\n",
      "Loss : 0.2969737946987152\n",
      "Loss : 0.2763586640357971\n",
      "Loss : 0.27480047941207886\n",
      "Loss : 0.36436572670936584\n",
      "Loss : 0.30996832251548767\n",
      "Loss : 0.25681784749031067\n",
      "Loss : 0.2293609231710434\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 38.00 MiB (GPU 0; 4.00 GiB total capacity; 3.27 GiB already allocated; 0 bytes free; 3.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(train_dl, val_dl, model, \u001b[39m5\u001b[39m, opt, loss_fn, device)\n\u001b[0;32m     29\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTraining Beginning\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m trainer\u001b[39m.\u001b[39;49mfit()\n\u001b[0;32m     32\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mTraining Complete\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39m'\u001b[39m\u001b[39m./models/DeePixBiS/DeePixBiS_celeb_nuaa_130223.pth\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\KhoiNXM\\Workspace\\Learning\\Master Thesis\\Dev\\face_recognition_system\\modules\\deep_pixel_wise\\Trainer.py:45\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     44\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs):\n\u001b[1;32m---> 45\u001b[0m     train_acc, train_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_one_epoch(epoch)\n\u001b[0;32m     46\u001b[0m     training_acc\u001b[39m.\u001b[39mappend(train_acc)\n\u001b[0;32m     47\u001b[0m     training_loss\u001b[39m.\u001b[39mappend(train_loss)\n",
      "File \u001b[1;32mc:\\KhoiNXM\\Workspace\\Learning\\Master Thesis\\Dev\\face_recognition_system\\modules\\deep_pixel_wise\\Trainer.py:34\u001b[0m, in \u001b[0;36mTrainer.train_one_epoch\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLoss : \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[39m# self.model.eval()\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m test_acc \u001b[39m=\u001b[39m test_accuracy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mval_dl, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[0;32m     35\u001b[0m test_los \u001b[39m=\u001b[39m test_loss(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_dl, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     37\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTest Accuracy : \u001b[39m\u001b[39m{\u001b[39;00mtest_acc\u001b[39m}\u001b[39;00m\u001b[39m  Test Loss : \u001b[39m\u001b[39m{\u001b[39;00mtest_los\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\KhoiNXM\\Workspace\\Learning\\Master Thesis\\Dev\\face_recognition_system\\modules\\deep_pixel_wise\\Metrics.py:24\u001b[0m, in \u001b[0;36mtest_accuracy\u001b[1;34m(model, test_dl, device)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m img, mask, label \u001b[39min\u001b[39;00m test_dl:\n\u001b[0;32m     23\u001b[0m     img, mask \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mto(device), mask\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 24\u001b[0m     net_mask, net_label \u001b[39m=\u001b[39m model(img)\n\u001b[0;32m     25\u001b[0m     preds, _ \u001b[39m=\u001b[39m predict(net_mask, net_label)\n\u001b[0;32m     26\u001b[0m     ac \u001b[39m=\u001b[39m (preds \u001b[39m==\u001b[39m label)\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mFloatTensor)\n",
      "File \u001b[1;32mc:\\KhoiNXM\\Workspace\\python_venv\\face_recognition_system\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\KhoiNXM\\Workspace\\Learning\\Master Thesis\\Dev\\face_recognition_system\\modules\\deep_pixel_wise\\Model.py:18\u001b[0m, in \u001b[0;36mDeePixBiS.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 18\u001b[0m     enc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menc(x)\n\u001b[0;32m     19\u001b[0m     dec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdec(enc)\n\u001b[0;32m     20\u001b[0m     out_map \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msigmoid(dec)\n",
      "File \u001b[1;32mc:\\KhoiNXM\\Workspace\\python_venv\\face_recognition_system\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\KhoiNXM\\Workspace\\python_venv\\face_recognition_system\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\KhoiNXM\\Workspace\\python_venv\\face_recognition_system\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\KhoiNXM\\Workspace\\python_venv\\face_recognition_system\\lib\\site-packages\\torchvision\\models\\densenet.py:123\u001b[0m, in \u001b[0;36m_DenseBlock.forward\u001b[1;34m(self, init_features)\u001b[0m\n\u001b[0;32m    121\u001b[0m features \u001b[39m=\u001b[39m [init_features]\n\u001b[0;32m    122\u001b[0m \u001b[39mfor\u001b[39;00m name, layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems():\n\u001b[1;32m--> 123\u001b[0m     new_features \u001b[39m=\u001b[39m layer(features)\n\u001b[0;32m    124\u001b[0m     features\u001b[39m.\u001b[39mappend(new_features)\n\u001b[0;32m    125\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(features, \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\KhoiNXM\\Workspace\\python_venv\\face_recognition_system\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\KhoiNXM\\Workspace\\python_venv\\face_recognition_system\\lib\\site-packages\\torchvision\\models\\densenet.py:89\u001b[0m, in \u001b[0;36m_DenseLayer.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     87\u001b[0m     bottleneck_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_checkpoint_bottleneck(prev_features)\n\u001b[0;32m     88\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 89\u001b[0m     bottleneck_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn_function(prev_features)\n\u001b[0;32m     91\u001b[0m new_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(bottleneck_output)))\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_rate \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\KhoiNXM\\Workspace\\python_venv\\face_recognition_system\\lib\\site-packages\\torchvision\\models\\densenet.py:50\u001b[0m, in \u001b[0;36m_DenseLayer.bn_function\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbn_function\u001b[39m(\u001b[39mself\u001b[39m, inputs: List[Tensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m     49\u001b[0m     concated_features \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(inputs, \u001b[39m1\u001b[39m)\n\u001b[1;32m---> 50\u001b[0m     bottleneck_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrelu1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm1(concated_features)))  \u001b[39m# noqa: T484\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     \u001b[39mreturn\u001b[39;00m bottleneck_output\n",
      "File \u001b[1;32mc:\\KhoiNXM\\Workspace\\python_venv\\face_recognition_system\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\KhoiNXM\\Workspace\\python_venv\\face_recognition_system\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\KhoiNXM\\Workspace\\python_venv\\face_recognition_system\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 0; 4.00 GiB total capacity; 3.27 GiB already allocated; 0 bytes free; 3.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model = DeePixBiS()\n",
    "# model.load_state_dict(torch.load('/content/drive/MyDrive/FPT MSE/Capstone Project/Anti_Spoof_DPW/DeePixBiS.pth'))\n",
    "loss_fn = PixWiseBCELoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "train_tfms = Compose([Resize([224, 224]),\n",
    "                      RandomHorizontalFlip(),\n",
    "                      RandomRotation(10),\n",
    "                      ToTensor(),\n",
    "                    #   Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "                      Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "test_tfms = Compose([Resize([224, 224]),\n",
    "                     ToTensor(),\n",
    "                    #  Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "                     Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "train_dataset = PixWiseDataset(train_data_shuffled, transform=train_tfms)\n",
    "train_ds = train_dataset.dataset()\n",
    "\n",
    "val_dataset = PixWiseDataset(val_data, transform=test_tfms)\n",
    "val_ds = val_dataset.dataset()\n",
    "\n",
    "batch_size = 16\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_dl = DataLoader(val_ds, batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "trainer = Trainer(train_dl, val_dl, model, 5, opt, loss_fn, device)\n",
    "\n",
    "print('Training Beginning\\n')\n",
    "trainer.fit()\n",
    "\n",
    "print('\\nTraining Complete')\n",
    "torch.save(model.state_dict(), './models/DeePixBiS/DeePixBiS_celeb_nuaa_130223.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1460649653.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[10], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    sub_train_data_part2 = train_data_shuffled[int(len(train_data_shuffled)*0.5:)].reset_index(drop=True)\u001b[0m\n\u001b[1;37m                                                                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "del model, train_dataset, train_ds, train_dl, val_dataset, val_ds, val_dl, trainer\n",
    "sub_train_data_part2 = train_data_shuffled[int(len(train_data_shuffled)*0.5:)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeePixBiS()\n",
    "model.load_state_dict(torch.load('/content/drive/MyDrive/FPT MSE/Capstone Project/Anti_Spoof_DPW/DeePixBiS.pth'))\n",
    "loss_fn = PixWiseBCELoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "train_tfms = Compose([Resize([224, 224]),\n",
    "                      RandomHorizontalFlip(),\n",
    "                      RandomRotation(10),\n",
    "                      ToTensor(),\n",
    "                    #   Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "                      Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "test_tfms = Compose([Resize([224, 224]),\n",
    "                     ToTensor(),\n",
    "                    #  Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "                     Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "train_dataset = PixWiseDataset(train_data_shuffled, transform=train_tfms)\n",
    "train_ds = train_dataset.dataset()\n",
    "\n",
    "val_dataset = PixWiseDataset(val_data, transform=test_tfms)\n",
    "val_ds = val_dataset.dataset()\n",
    "\n",
    "batch_size = 16\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_dl = DataLoader(val_ds, batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "trainer = Trainer(train_dl, val_dl, model, 5, opt, loss_fn, device)\n",
    "\n",
    "print('Training Beginning\\n')\n",
    "trainer.fit()\n",
    "\n",
    "print('\\nTraining Complete')\n",
    "torch.save(model.state_dict(), './models/DeePixBiS/DeePixBiS_celeb_nuaa_130223.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_recognition_system",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2b9d8e355d89e6612b110ab03e3987dce92c37600e380ef8505b30519cf4243"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
