{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Compose, ToTensor, RandomHorizontalFlip, Normalize, Resize, RandomRotation\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from modules.deep_pixel_wise.Dataset import PixWiseDataset\n",
    "from modules.deep_pixel_wise.Model import DeePixBiS\n",
    "from modules.deep_pixel_wise.Loss import PixWiseBCELoss\n",
    "from modules.deep_pixel_wise.Metrics import predict, test_accuracy, test_loss\n",
    "from modules.deep_pixel_wise.Trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('.\\data\\\\path_files\\\\train_data_celeb_nuaa.csv')\n",
    "test_data = pd.read_csv('.\\data\\\\path_files\\\\test_data_celeb_nuaa.csv')\n",
    "val_data = pd.read_csv('.\\data\\\\path_files\\\\val_data_celeb_nuaa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_shuffled = train_data.sample(frac=1)\n",
    "train_data_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\KhoiNXM\\Workspace\\python_venv\\face_recognition_system\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet161_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet161_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Training Beginning\n",
      "\n",
      "\n",
      "Epoch (1/5)\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\KhoiNXM\\Workspace\\python_venv\\face_recognition_system\\lib\\site-packages\\torch\\nn\\functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 0.692446231842041\n",
      "Loss : 0.7081700563430786\n",
      "Loss : 0.675376296043396\n",
      "Loss : 0.599183201789856\n",
      "Loss : 0.6736254692077637\n",
      "Loss : 0.623955249786377\n",
      "Loss : 0.588654637336731\n",
      "Loss : 0.5497281551361084\n",
      "Loss : 0.5760930180549622\n",
      "Loss : 0.5453912019729614\n",
      "Loss : 0.6515114307403564\n",
      "Loss : 0.5268553495407104\n",
      "Loss : 0.4775044918060303\n",
      "Loss : 0.4508039355278015\n",
      "Loss : 0.500619113445282\n",
      "Loss : 0.47558730840682983\n",
      "Loss : 0.5101454854011536\n",
      "Loss : 0.4464538097381592\n",
      "Loss : 0.47487056255340576\n",
      "Loss : 0.5162633657455444\n",
      "Loss : 0.4001259505748749\n",
      "Loss : 0.3983888030052185\n",
      "Loss : 0.5717832446098328\n",
      "Loss : 0.361125648021698\n",
      "Loss : 0.3923700451850891\n",
      "Loss : 0.36822760105133057\n",
      "Loss : 0.45132946968078613\n",
      "Loss : 0.43708837032318115\n",
      "Loss : 0.35730814933776855\n",
      "Loss : 0.35720309615135193\n",
      "Loss : 0.3217101991176605\n",
      "Loss : 0.3503861427307129\n",
      "Loss : 0.4788060784339905\n",
      "Loss : 0.5505707263946533\n",
      "Loss : 0.3017981946468353\n",
      "Loss : 0.3290308117866516\n",
      "Loss : 0.3227561116218567\n",
      "Loss : 0.3438206613063812\n",
      "Loss : 0.27922090888023376\n",
      "Loss : 0.29474014043807983\n",
      "Loss : 0.26339590549468994\n",
      "Loss : 0.3073248267173767\n",
      "Loss : 0.7456119060516357\n",
      "Loss : 0.28213635087013245\n",
      "Loss : 0.27589038014411926\n",
      "Loss : 0.26562780141830444\n",
      "Loss : 0.2815471589565277\n",
      "Loss : 0.280200719833374\n",
      "Loss : 0.2680138945579529\n",
      "Loss : 0.28395548462867737\n",
      "Loss : 0.24391117691993713\n",
      "Loss : 0.24176540970802307\n",
      "Loss : 0.2162211835384369\n",
      "Loss : 0.3043752908706665\n",
      "Loss : 0.3296457827091217\n",
      "Loss : 0.2429088056087494\n",
      "Loss : 0.2594371736049652\n",
      "Loss : 0.2548309564590454\n",
      "Loss : 0.26356685161590576\n",
      "Loss : 0.25759661197662354\n",
      "Loss : 0.20517155528068542\n",
      "Loss : 0.3562372922897339\n",
      "Loss : 0.2561023533344269\n",
      "Loss : 0.2287537157535553\n",
      "Loss : 0.23674046993255615\n",
      "Loss : 0.23314893245697021\n",
      "Loss : 0.2886106073856354\n",
      "Loss : 0.23391148447990417\n",
      "Loss : 0.23233594000339508\n",
      "Loss : 0.20214033126831055\n",
      "Loss : 0.22403912246227264\n",
      "Loss : 0.20644895732402802\n",
      "Loss : 0.23358896374702454\n",
      "Loss : 0.1919964849948883\n",
      "Loss : 0.20880462229251862\n",
      "Loss : 0.23822729289531708\n",
      "Loss : 0.18735083937644958\n",
      "Loss : 0.2103588581085205\n",
      "Loss : 0.23773455619812012\n",
      "Loss : 0.21448512375354767\n",
      "Loss : 0.20690150558948517\n",
      "Loss : 0.2736719846725464\n",
      "Loss : 0.18715426325798035\n",
      "Loss : 0.24660751223564148\n",
      "Loss : 0.16436566412448883\n",
      "Loss : 0.24071872234344482\n",
      "Loss : 0.35528063774108887\n",
      "Loss : 0.21079009771347046\n",
      "Loss : 0.3031767010688782\n",
      "Loss : 0.43549415469169617\n",
      "Loss : 0.2522584795951843\n",
      "Loss : 0.20767280459403992\n",
      "Loss : 0.1758107841014862\n",
      "Loss : 0.17328371107578278\n",
      "Loss : 0.1760890781879425\n",
      "Loss : 0.22757023572921753\n",
      "Loss : 0.17844317853450775\n",
      "Loss : 0.1427432894706726\n",
      "Loss : 0.23381759226322174\n",
      "Loss : 0.24832196533679962\n",
      "Loss : 0.22309662401676178\n",
      "Loss : 0.17727896571159363\n",
      "Loss : 0.17661575973033905\n",
      "Loss : 0.2753545641899109\n",
      "Loss : 0.14763769507408142\n",
      "Loss : 0.21597470343112946\n",
      "Loss : 0.19259443879127502\n",
      "Loss : 0.17388880252838135\n",
      "Loss : 0.13693204522132874\n",
      "Loss : 0.16893649101257324\n",
      "Loss : 0.16711512207984924\n",
      "Loss : 0.14011842012405396\n",
      "Loss : 0.1919509768486023\n",
      "Loss : 0.19753625988960266\n",
      "Loss : 0.16077043116092682\n",
      "Loss : 0.24046072363853455\n",
      "Loss : 0.1499747931957245\n",
      "Loss : 0.1990586817264557\n",
      "Loss : 0.17123368382453918\n",
      "Loss : 0.20038622617721558\n",
      "Loss : 0.12629280984401703\n",
      "Loss : 0.16716638207435608\n",
      "Loss : 0.18082815408706665\n",
      "Loss : 0.3159584403038025\n",
      "Loss : 0.1957056075334549\n",
      "Loss : 0.15912294387817383\n",
      "Loss : 0.14800883829593658\n",
      "Loss : 0.2917272746562958\n",
      "Loss : 0.15763139724731445\n",
      "Loss : 0.1488254964351654\n",
      "Loss : 0.14569133520126343\n",
      "Loss : 0.1810036450624466\n",
      "Loss : 0.13167572021484375\n",
      "Loss : 0.3223055601119995\n",
      "Loss : 0.14165130257606506\n",
      "Loss : 0.15972542762756348\n",
      "Loss : 0.14345332980155945\n",
      "Loss : 0.22487998008728027\n",
      "Loss : 0.2685394287109375\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 38.00 MiB (GPU 0; 4.00 GiB total capacity; 3.26 GiB already allocated; 0 bytes free; 3.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(train_dl, val_dl, model, \u001b[39m5\u001b[39m, opt, loss_fn, device)\n\u001b[0;32m     35\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTraining Beginning\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m trainer\u001b[39m.\u001b[39;49mfit()\n\u001b[0;32m     38\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mTraining Complete\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     39\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39m'\u001b[39m\u001b[39m./models/DeePixBiS/DeePixBiS_celeb_nuaa_260123.pth\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\KhoiNXM\\Workspace\\Learning\\Master Thesis\\Dev\\face_recognition_system\\modules\\deep_pixel_wise\\Trainer.py:45\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     44\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs):\n\u001b[1;32m---> 45\u001b[0m     train_acc, train_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_one_epoch(epoch)\n\u001b[0;32m     46\u001b[0m     training_acc\u001b[39m.\u001b[39mappend(train_acc)\n\u001b[0;32m     47\u001b[0m     training_loss\u001b[39m.\u001b[39mappend(train_loss)\n",
      "File \u001b[1;32mc:\\KhoiNXM\\Workspace\\Learning\\Master Thesis\\Dev\\face_recognition_system\\modules\\deep_pixel_wise\\Trainer.py:34\u001b[0m, in \u001b[0;36mTrainer.train_one_epoch\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLoss : \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[39m# self.model.eval()\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m test_acc \u001b[39m=\u001b[39m test_accuracy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mval_dl)\n\u001b[0;32m     35\u001b[0m test_los \u001b[39m=\u001b[39m test_loss(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_dl, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn)\n\u001b[0;32m     37\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTest Accuracy : \u001b[39m\u001b[39m{\u001b[39;00mtest_acc\u001b[39m}\u001b[39;00m\u001b[39m  Test Loss : \u001b[39m\u001b[39m{\u001b[39;00mtest_los\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\KhoiNXM\\Workspace\\Learning\\Master Thesis\\Dev\\face_recognition_system\\modules\\deep_pixel_wise\\Metrics.py:25\u001b[0m, in \u001b[0;36mtest_accuracy\u001b[1;34m(model, test_dl)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m img, mask, label \u001b[39min\u001b[39;00m test_dl:\n\u001b[0;32m     24\u001b[0m     img, mask \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mto(device), mask\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 25\u001b[0m     net_mask, net_label \u001b[39m=\u001b[39m model(img)\n\u001b[0;32m     26\u001b[0m     preds, _ \u001b[39m=\u001b[39m predict(net_mask, net_label)\n\u001b[0;32m     27\u001b[0m     ac \u001b[39m=\u001b[39m (preds \u001b[39m==\u001b[39m label)\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mFloatTensor)\n",
      "File \u001b[1;32mc:\\KhoiNXM\\Workspace\\python_venv\\face_recognition_system\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\KhoiNXM\\Workspace\\Learning\\Master Thesis\\Dev\\face_recognition_system\\modules\\deep_pixel_wise\\Model.py:18\u001b[0m, in \u001b[0;36mDeePixBiS.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 18\u001b[0m     enc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menc(x)\n\u001b[0;32m     19\u001b[0m     dec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdec(enc)\n\u001b[0;32m     20\u001b[0m     out_map \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msigmoid(dec)\n",
      "File \u001b[1;32mc:\\KhoiNXM\\Workspace\\python_venv\\face_recognition_system\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\KhoiNXM\\Workspace\\python_venv\\face_recognition_system\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\KhoiNXM\\Workspace\\python_venv\\face_recognition_system\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\KhoiNXM\\Workspace\\python_venv\\face_recognition_system\\lib\\site-packages\\torchvision\\models\\densenet.py:123\u001b[0m, in \u001b[0;36m_DenseBlock.forward\u001b[1;34m(self, init_features)\u001b[0m\n\u001b[0;32m    121\u001b[0m features \u001b[39m=\u001b[39m [init_features]\n\u001b[0;32m    122\u001b[0m \u001b[39mfor\u001b[39;00m name, layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems():\n\u001b[1;32m--> 123\u001b[0m     new_features \u001b[39m=\u001b[39m layer(features)\n\u001b[0;32m    124\u001b[0m     features\u001b[39m.\u001b[39mappend(new_features)\n\u001b[0;32m    125\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(features, \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\KhoiNXM\\Workspace\\python_venv\\face_recognition_system\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\KhoiNXM\\Workspace\\python_venv\\face_recognition_system\\lib\\site-packages\\torchvision\\models\\densenet.py:89\u001b[0m, in \u001b[0;36m_DenseLayer.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     87\u001b[0m     bottleneck_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_checkpoint_bottleneck(prev_features)\n\u001b[0;32m     88\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 89\u001b[0m     bottleneck_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn_function(prev_features)\n\u001b[0;32m     91\u001b[0m new_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(bottleneck_output)))\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_rate \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\KhoiNXM\\Workspace\\python_venv\\face_recognition_system\\lib\\site-packages\\torchvision\\models\\densenet.py:50\u001b[0m, in \u001b[0;36m_DenseLayer.bn_function\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbn_function\u001b[39m(\u001b[39mself\u001b[39m, inputs: List[Tensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m     49\u001b[0m     concated_features \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(inputs, \u001b[39m1\u001b[39m)\n\u001b[1;32m---> 50\u001b[0m     bottleneck_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrelu1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm1(concated_features)))  \u001b[39m# noqa: T484\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     \u001b[39mreturn\u001b[39;00m bottleneck_output\n",
      "File \u001b[1;32mc:\\KhoiNXM\\Workspace\\python_venv\\face_recognition_system\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\KhoiNXM\\Workspace\\python_venv\\face_recognition_system\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\KhoiNXM\\Workspace\\python_venv\\face_recognition_system\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 0; 4.00 GiB total capacity; 3.26 GiB already allocated; 0 bytes free; 3.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model = DeePixBiS()\n",
    "# model.load_state_dict(torch.load('/content/drive/MyDrive/FPT MSE/Capstone Project/Anti_Spoof_DPW/DeePixBiS.pth'))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "loss_fn = PixWiseBCELoss()\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "train_tfms = Compose([Resize([224, 224]),\n",
    "                      RandomHorizontalFlip(),\n",
    "                      RandomRotation(10),\n",
    "                      ToTensor(),\n",
    "                    #   Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "                      Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "test_tfms = Compose([Resize([224, 224]),\n",
    "                     ToTensor(),\n",
    "                    #  Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "                     Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "train_dataset = PixWiseDataset('.\\data\\\\path_files\\\\train_data_celeb_nuaa.csv', transform=train_tfms)\n",
    "train_ds = train_dataset.dataset()\n",
    "\n",
    "val_dataset = PixWiseDataset('.\\data\\\\path_files\\\\val_data_celeb_nuaa.csv', transform=test_tfms)\n",
    "val_ds = val_dataset.dataset()\n",
    "\n",
    "batch_size = 16\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_dl = DataLoader(val_ds, batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "trainer = Trainer(train_dl, val_dl, model, 5, opt, loss_fn, device)\n",
    "\n",
    "print('Training Beginning\\n')\n",
    "trainer.fit()\n",
    "\n",
    "print('\\nTraining Complete')\n",
    "torch.save(model.state_dict(), './models/DeePixBiS/DeePixBiS_celeb_nuaa_260123.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_recognition_system",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (tags/v3.9.12:b28265d, Mar 23 2022, 23:52:46) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2b9d8e355d89e6612b110ab03e3987dce92c37600e380ef8505b30519cf4243"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
